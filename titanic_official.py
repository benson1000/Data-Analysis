# -*- coding: utf-8 -*-
"""Titanic_official

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GPASvWqX7fW9dpAwfj2xRLOSRrvskPYP

## **Titanic - Machine Learning from Disaster**

![download.jpg](data:image/jpeg;base64,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)

**The RMS Titanic, a luxury steamship, sank in the early hours of April 15, 1912, off the coast of Newfoundland in the North Atlantic after sideswiping an iceberg during its maiden voyage. Of the 2,240 passengers and crew on board, more than 1,500 lost their lives in the disaster.**

Prediction of survival on the Titanic
"""

# Commented out IPython magic to ensure Python compatibility.
import os    #it interacts with the underlying operating system.
import numpy as np   #for linear algebra and mathematical calculation.
import pandas as pd   #for data manipulation and analysis
import matplotlib.pyplot as plt  #for plotting
# %matplotlib inline
import seaborn as sns   #this is also for plotting the dataset.
plt.style.use("ggplot")   #the style to use in plotting
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.float_format', '{:.3f}'.format)    #importing the libraries for analyisis

#pip install missingno   #this is for visualizing missing values in the dataset.

#pip install matplotlib==3.4.2   #install this version of matplotlib for plotting the graphs.

import matplotlib
matplotlib.__version__   #checking the version

os.getcwd()     #getting the current working directory

import zipfile   #this is for extracting the data

def FileExtraction():
    with zipfile.ZipFile('/content/titanic.zip') as file:
        print(file.infolist())
        print('\n')
        print(file.namelist())
        print('\n')
        print(file.extractall())
if __name__ =='__main__':FileExtraction()   #Extracting the zipfile

train_dataset = pd.read_csv('train.csv') #reading the train dataset
test_data=pd.read_csv("test.csv")  #reading the test dataset

train_dataset.head() # displays the first five rows of the data

train_dataset.tail()  #displays the last five rows of the data

test_data.head()    #the first five rows of the test dataset.

train_dataset.shape  #the shape of the dataset, the train dataset and the test dataset.

train_dataset.dtypes.value_counts()  #it returns the number of occurances of each datatype in the dataset.

train_dataset.columns   #it displays the columns in the dataset.

train_dataset.nunique()   #the number of unique values per column in the entire dataset

train_dataset.info()  #it provides information about the dataset. This information includes the Non-missing values
                       #per column, the columns datatype and the columns

train_dataset.describe().style.background_gradient(cmap='twilight_r')   #A summary of descriptive statistics about the dataset. This summary only uses numeric dataset.

train_dataset.isnull().sum()  #the sum of missing values per column

"""TEST DATA"""

test_data.shape  #the shape of the test daaset

test_data.columns   #the columns in the test data.

test_data.dtypes.value_counts()    #the data types in the dataset.

test_data.size    #the size of the data.

test_data.nunique()    #the number of unique values per column in the dataset.

test_data.info()   #provides information about the test dataset.

test_data.isnull().sum()   #the sum of missing values per column in the dataset

"""# DATA PREPROCESSING and DATA CLEANING

Setting the PassengerID as the index
"""

train_dataset.set_index('PassengerId',inplace=True)
train_dataset.index.name=None   #index to have no name

train_dataset.head()

test_data.set_index('PassengerId', inplace=True)
test_data.index.name=None #the index of the data to have no index name

test_data.head()

"""Summary Statistics per survival categort"""

train_dataset[train_dataset['Survived']==0].describe()   #statistics of the dataset where the Surived =0

train_dataset[train_dataset['Survived']==1].describe().style.set_properties(backgroundcolor='black',color='white')

"""Checking and Dropping Duplicates in the dataset"""

train_dataset.duplicated().sum()   #sum of duplicates in the training dataset.

train_dataset.drop_duplicates(inplace=True)  #dropping missing values in the training dataset

test_data.duplicated().sum() #sum of duplicates in the test dataset.

test_data.drop_duplicates(inplace=True)   #dropping duplicates in the test dataset

"""Creating columns for analysis"""

train_dataset.head()

train_dataset['Family_Size'] = train_dataset['SibSp'] + train_dataset['Parch']
plt.figure(figsize=(12,10))
sns.countplot(data=train_dataset,x='Family_Size')
plt.show()

"""The Correlation among the variables"""

corr_matrix=train_dataset.corr()
corr_matrix   #the standard correlation coefficient.

corr_matrix['Survived'].sort_values(ascending=False)   #how the features relates with the target variable

from pandas.plotting import scatter_matrix
scatter_matrix(train_dataset, figsize=(12,10),color='blue')  #graph of how each features relates to each other
plt.show()

plt.figure(figsize=(12,10))
sns.heatmap(corr_matrix,annot=True,vmin=-1,vmax=1,fmt='0.1%',cmap='viridis')   #visual of the correlation in % form
plt.show()

"""Checking and working with OUTLIERS"""

iqr_factor = [1.5, 2]
list1, list2 = [], []

for factor in iqr_factor:
    counts = 0
    print(f'Outliers for {factor} IQR :')
    for col in train_dataset.select_dtypes(['int64', 'float64']):

      IQR = train_dataset[col].quantile(0.75) - train_dataset[col].quantile(0.25)
      lower_lim = train_dataset[col].quantile(0.25) - factor*IQR
      upper_lim = train_dataset[col].quantile(0.75) + factor*IQR

      cond = train_dataset[(train_dataset[col] < lower_lim) | (train_dataset[col] > upper_lim)].shape[0]
      if cond > 0 and factor == 1.5:
        list1.append(train_dataset[(train_dataset[col] < lower_lim) | (train_dataset[col] > upper_lim)].index.tolist())
      elif cond > 0 and factor == 2:
        list2.append(train_dataset[(train_dataset[col] < lower_lim) | (train_dataset[col] > upper_lim)].index.tolist())

      if cond > 0: print(f'{col:<30} : ', cond); counts += cond
print(f'\n TOTAL OUTLIERS FOR {factor} IQR : {counts}')
print('')

fig,ax=plt.subplots(2,2,figsize=(20,20))
sns.boxplot(data=train_dataset,x='Survived',y='Parch',ax=ax[0,0])
sns.boxplot(data=train_dataset,x='Survived',y='SibSp',ax=ax[0,1])
sns.boxplot(data=train_dataset,x='Survived',y='Age',ax=ax[1,0])
sns.boxplot(data=train_dataset,x='Survived',y='Fare',ax=ax[1,1])
plt.tight_layout(), plt.show()    #VISUALIZING THE OUTLIERS

mean = np.mean(train_dataset)

import missingno as msno
msno.bar(train_dataset)  #visualizing the missing columns in the dataset

train_dataset.isnull().sum()   #missing values in the training dataset.

"""Embarked point of the passengers ('Cherbourg':'C',       'Queenstown':'Q',     'Southampton':'S')"""

train_dataset[train_dataset['Embarked'].isna()]     #the rows where the Embarked column is missing values.

plt.figure(figsize=(18,12))
color=['green','blue','#34495E']

plt.subplot(1,2,1)
ax2 = plt.bar(train_dataset['Embarked'].value_counts().index,train_dataset['Embarked'].value_counts().values,label='Embarked',color=color)
plt.bar_label(ax2,size=16)
plt.xlabel('Embarked'),plt.ylabel('Number of Passengers'), plt.xticks([0,1,2])
plt.title('Embarkation Port Distribution',color='white',backgroundcolor='k',size=18)

plt.subplot(1,2,2)
plot1=sns.countplot(data=train_dataset,x='Embarked',hue='Survived',palette='viridis')
plot1.bar_label(plot1.containers[0],color='orange',size=17), plot1.bar_label(plot1.containers[1],color='brown',size=20)

plt.title('Embarkation by Survival',fontfamily='Times New Roman',size=25,color='white',backgroundcolor='k')
plt.show()

#Filling Embarked column missing values with the mode (Most occurances)
print("Before filling the missing values:\n",train_dataset['Embarked'].value_counts(dropna=False))
print('\n')
train_dataset['Embarked'].fillna(value=train_dataset['Embarked'].mode()[0],inplace=True)
print("After filling the missing values:\n",train_dataset['Embarked'].value_counts())

"""Fare of the passengers"""

plt.figure(figsize=(12,10))
plt.title("Distribution of Fare")
sns.histplot(data=train_dataset,x="Fare",hue="Survived",bins=30)
plt.show()

train_dataset.groupby('Survived')['Fare'].describe()  #statistics of Fare attribute grouped by survived

test_data[test_data['Fare'].isna()]   #the missing Fare value in the test data

df=test_data[(test_data['Pclass']==3) & (test_data['Embarked']=='S')]   #Filtering the data so as to impute the missing value
df.describe()    #summary statistics about the data

median=df.describe()['Fare']['50%']    #calculating the median of the data
test_data['Fare'].fillna(value=median,inplace=True)   #filling the missing values

"""Ticket of the passengers"""

train_dataset['Ticket'].value_counts()  #Tickets are unique as each passenger has its own ticket thus the tickets must be unique

non_unque_tickets=train_dataset['Ticket'].value_counts()[train_dataset['Ticket'].value_counts().values>1]
non_unque_tickets.index   #the tickets that are not unique

unique_tickets=train_dataset['Ticket'].value_counts()[train_dataset['Ticket'].value_counts().values==1]
unique_tickets.index     #the tickets that are unique.

train_dataset['Ticket'].value_counts()[train_dataset['Ticket'].value_counts().values>1]

plt.figure(figsize=(20,8))
plt.subplot(1,2,1)
ax9=sns.countplot(data=train_dataset[(train_dataset['Family_Size']>0) &(train_dataset['Ticket'].isin(unique_tickets.index))],
              x='Sex',hue='Survived')
ax9.bar_label(ax9.containers[0],color='orange',size=17), ax9.bar_label(ax9.containers[1],color='brown',size=20)
plt.show()

train_dataset['Ticket'] = train_dataset['Ticket'].apply(lambda x: 'own' if x in unique_tickets else 'collective')
test_data['Ticket'] = test_data['Ticket'].apply(lambda x: 'own' if x in unique_tickets else 'collective')



"""Age column"""

train_dataset['Age'].isnull().sum(), test_data['Age'].isnull().sum()  #missing values for both train and test dataset

plt.figure(figsize=(15,10))
sns.histplot(data=train_dataset,x='Age',hue='Survived',palette='winter_r')
plt.title("Age Distribution",fontfamily="Times New Roman",size=25)
plt.show()

train_dataset[train_dataset['Age'].isna()].head()   #returns the data where the Age column is missing

train_dataset[train_dataset['Age'].isna()][['SibSp','Parch']].describe()

#filtering the data and grouping the filtered the data based on the SibSp and Parch
train_dataset[train_dataset['Age'].isna()].groupby(['SibSp','Parch'])['Age'].value_counts(dropna=False)

data_group=train_dataset.groupby(['SibSp','Parch','Pclass'])['Age'].mean()
data_group

#filling the missing values with the mean of the dataset according to the data group it belongs to
train_dataset['Age'].fillna(value=train_dataset.groupby(['SibSp','Parch','Pclass'])['Age'].transform('mean'),inplace=True)

#the only missing values in our age dataset
mean=train_dataset['Age'].describe()['mean']
train_dataset['Age'].fillna(value=mean,inplace=True)

train_dataset['Age'].isnull().sum()  #no mising values in the training dataset.

"""Working with missing values in the Age column of test dataset"""

test_data['Age'].value_counts(dropna=False)    #missing values in the test dataset

#filtering the data and grouping the missing data based on the SibSp and Parch features
test_data[test_data['Age'].isna()].groupby(['SibSp','Parch'])['Age'].value_counts(dropna=False)

test_data.groupby(['SibSp','Parch','Pclass'])['Age'].mean()

test_data['Age'].fillna(value=test_data.groupby(['SibSp','Parch','Pclass'])['Age'].transform('mean'),inplace=True)

#filling other missing values
mean=test_data['Age'].describe()['mean']
test_data['Age'].fillna(value=mean,inplace=True)

test_data['Age'].isnull().sum()

"""Drop the columns"""

train_dataset.drop(['Cabin','Name'],axis=1,inplace=True)    #outlier columns

"""Sex / Gender of the passengers"""

plt.figure(figsize=(15,8))
plt.subplot(1,2,1)
ax3=sns.countplot(data=train_dataset,x="Sex")
ax3.bar_label(ax3.containers[0],size=17)
plt.title(' Sex',fontfamily='Times New Roman',size=25,color='white',backgroundcolor='k')

plt.subplot(1,2,2)
ax4=sns.countplot(data=train_dataset,x="Sex",hue="Survived")
ax4.bar_label(ax4.containers[0],size=16), ax4.bar_label(ax4.containers[1],size=16,color='green')
plt.title('Sex by Survival',fontfamily='Times New Roman',size=25,color='white',backgroundcolor='k'),ax4.set_ylim(0,500)
plt.show()

"""Survived"""

labels=train_dataset['Survived'].value_counts().index
vals=train_dataset['Survived'].value_counts().values

plt.figure(figsize=(18,18))
plt.subplot(2,2,1)
ax8=sns.countplot(data=train_dataset,x="Survived")
ax8.bar_label(ax8.containers[0],color='green',size=18)

plt.subplot(2,2,2)
plt.pie(vals,autopct='%1.1f%%',frame=True,shadow=True,wedgeprops={'edgecolor':'k'},
        textprops={'color':'white','size':22,'backgroundcolor':'black'})
plt.title('Survived',color='green',size=25),plt.axis('equal'),plt.xticks(()),plt.yticks(()),plt.legend(labels,loc=1,fontsize=15)


plt.tight_layout()
plt.show()

"""Pclass:   TICKET CLASS: (1:'1st', 2:'2nd', 3:'3rd')"""

train_dataset['Pclass'].value_counts()  #the number of occuranves of each ticket class among the passengers.

plt.figure(figsize=(18,18))
plt.subplot(2,2,1)
color=['seagreen','#34495E', 'brown']
ax5 = plt.bar(train_dataset['Pclass'].value_counts().index,train_dataset['Pclass'].value_counts().values,label='Pclass',color=color)
plt.bar_label(ax5,size=17,color='black'), plt.xticks([1,2,3])
plt.title('Distribution of Ticket class',fontfamily='Times New Roman',size=25,color='white',backgroundcolor='k')

plt.subplot(2,2,2)
ax6=sns.countplot(data=train_dataset,x='Pclass',hue='Survived',palette='winter_r')
ax6.bar_label(ax6.containers[0],size=17,color='k'),ax6.bar_label(ax6.containers[1],size=17,color='red')
plt.title('Ticket Class by Survival',fontfamily='Times New Roman',size=25,color='white',backgroundcolor='k')
plt.xticks(color='k',size=17), plt.xlabel('Ticket Class',color='green',size=20)
plt.show()

plt.figure(figsize=(14,8))
sns.boxplot(data=train_dataset,x='Sex',y='Pclass',hue='Survived')
plt.show()

"""Parch: Number of parents and children of the passenger aboard"""

plt.figure(figsize=(12,12))
ax7=sns.countplot(data=train_dataset,x="Parch",hue='Survived',palette='autumn_r')
ax7.bar_label(ax7.containers[0],size=16), ax7.bar_label(ax7.containers[1],size=16), plt.legend(loc=1)
plt.show()

"""SibSp: Number of siblings and spouses of the passenger aboard"""

plt.figure(figsize=(12,10))
sns.countplot(data=train_dataset,x="SibSp",hue="Survived",palette='summer_r'), plt.legend(loc=1)
plt.show()

train_dataset.groupby('Survived').count()   #grouping the data by the target.

train_dataset.drop('Family_Size',axis=1,inplace=True)  #dropping the column created for analysis

"""# FEATURE ENGINEERING

Encoding the Sex column using get dummies.
"""

Sex_encoded=pd.get_dummies(train_dataset['Sex'],prefix='Sex') #converting the object feature sex into numerical using pandas.get_dummies()

train_dataset=pd.concat([train_dataset,Sex_encoded],axis=1)   #concating the two dataframes together using the pandas.concat()
train_dataset.drop('Sex',inplace=True,axis=1)  #dropping the object featuire

train_dataset.head()   #displaying the first five rows of the dataset after encoding.

"""Encoding the Embarked column"""

from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer  #importing the libraries.
encoder=LabelEncoder() #using the sklearn labelEncoder to convert object datatype to numerical datatype
train_dataset['Embarked']=encoder.fit_transform(train_dataset['Embarked'])

binarizer=LabelBinarizer()  #using the sklearn labelbinarizer to convert object datatype to numerical datatype
train_dataset['Ticket']=binarizer.fit_transform(train_dataset['Ticket'])

train_dataset.head()

test_data['Ticket']=binarizer.fit_transform(test_data['Ticket'])  #this converts the numerical datatype into onehot numericals

test_encoded=pd.get_dummies(test_data['Sex'],prefix='Sex')  #converting the sex dtatype into numerical datatype
test_data=pd.concat([test_data,test_encoded],axis=1)  #joining the two dataframes togerher by using the concat pf pandas.
test_data.drop('Sex',inplace=True,axis=1)  #dropping the object feature

plt.figure(figsize=(16,10))
sns.heatmap(train_dataset.corr(),annot=True,fmt='0.1%',cmap='viridis')  #the correlation after the above steps, data cleaning and feature engineering.
plt.show()

plt.figure(figsize=(10,8))
corr=train_dataset.corr().drop('Survived')
corr['Survived'].sort_values(ascending=True).plot.barh(color='blue')

"""SPLITTING THE DATASET"""

from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV, StratifiedKFold, cross_val_predict
X = train_dataset.drop('Survived',axis=1)  #this is a Matrix
y=train_dataset['Survived']   #this is a vector

X.columns   #the columns in the X dataframe

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)  #splitting the dataset

print("The shape of the training dataset is:\t",X_train.shape)   #the shape of the training dataset.
print("The shape of the test dataset is:\t",X_test.shape)   #shaoe of the testing dataset.

y.value_counts() #the data is imbalanced with 0 having majority values while y has minority thus minority class.

scaler=StandardScaler()   #initializing the method
X_train=scaler.fit_transform(X_train)  #NORMALIZING the data to a particular range/scale
X_test=scaler.transform(X_test)

"""# MODEL CREATION"""

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn import metrics

n_folds=StratifiedKFold(n_splits=10)  #Creating strata groups on the dataset.
def evaluation(model, X_train, y_train, X_test, y_test):   #creating a function called evaluation and passing the parameters as set above.
    y_predicted=model.predict(X_test)    #the miodel passed here predicts the output.
    print("The confusion matrix is:", metrics.confusion_matrix(y_test,y_predicted))   #the confusion matrix having FN,TN,FP and TP
    print("\n")
    print("The Classification report is:",metrics.classification_report(y_test, y_predicted))  #the classification report

"""# KNEARESTNEIGHBORSCLASSIFIER"""

knn_model=KNeighborsClassifier()   #initializing the Kneighbors classifier for classification
knn_model.fit(X_train,y_train)   #training the algorithm with the training dataset.

evaluation(knn_model,X_train,y_train,X_test,y_test)   #calling the fuction with the knn classifier model.

cross_val_score(knn_model,X_train,y_train,cv=n_folds,scoring='accuracy').mean() #the training accuracy of the knn model

cross_val_score(knn_model,X_train,y_train,cv=n_folds,scoring='precision').mean() #the training precision of the knn model

"""Model Optimization"""

param_grid={'n_neighbors':np.arange(0,30),
           'weights':['uniform','distance'],
           'algorithm':['auto','ball_tree','kd_tree','brute'],
           'leaf_size':np.arange(0,11)}

#using gridsearchcv for finding the best parameters for the model to increase the accuracy of tyhe model.
knn_grid=GridSearchCV(knn_model,param_grid=param_grid,n_jobs=-1,cv=n_folds,scoring='accuracy')

# Commented out IPython magic to ensure Python compatibility.
import time
# %time knn_grid.fit(X_train,y_train) #fitting the new knn model with the training dataset and getting the time used to run

print("The best parameter is:\t",knn_grid.best_params_) #obtaining the best parameters based on the results of the GridSearchCV
print('\n')
print("The best estimator is:\t",knn_grid.best_estimator_) #obtaining the best ESTIMATOR based On the results of the GridSearchCV
print('\n')
print("The best score is:\t",knn_grid.best_score_) #obtaining the best score of the model based on the results of the GridSearchCV

evaluation(knn_grid,X_train,y_train,X_test,y_test) #the evaluation after the gridsearchcv

knn_grid_model=knn_grid.best_estimator_
knn_grid_model.fit(X_train,y_train)

cross_val_score(knn_grid_model,X_train,y_train,cv=n_folds,scoring='accuracy').mean() #training accuracy after obtaining the best estimator and using it.

cross_val_score(knn_grid_model,X_train,y_train,cv=n_folds,scoring='precision').mean()  #precision score after obtaining the best estimators

y_train_predict=knn_grid_model.predict(X_train)   #predicting the training dataset
knn_training_accuracy=cross_val_score(knn_grid_model,X_train,y_train,cv=n_folds,scoring='accuracy').mean() #training accuracy using cross-validation
knn_train_precision=metrics.precision_score(y_train,y_train_predict)  #precision scopre of the training dataset.

y_pred=knn_grid_model.predict(X_test)  #predicting a new dataset to validate the model efficiency.
knn_test_accuracy=metrics.accuracy_score(y_test,y_pred)   #getting the test accuracy of the model on the new dataset
knn_test_precision =metrics.precision_score(y_test,y_pred)  #getting the test precision score of the model on the new dataset

"""# DECISION TREE CLASSIFIER"""

decision_tree=tree.DecisionTreeClassifier(random_state=42)#initializing the decuision tree classifier for classification
decision_tree.fit(X_train,y_train) #training the algorithm with the training dataset.

evaluation(decision_tree,X_train,y_train,X_test,y_test)   #calling the fuction with the decision tree classifier model.

cross_val_score(decision_tree,X_train,y_train, cv=n_folds,n_jobs=-1,scoring='accuracy').mean()
 #the training accuracy of the model before obtaining the best parameters

cross_val_score(decision_tree,X_train,y_train,cv=n_folds,scoring='precision').mean()
#the training precision of the model before obtaining the best parameters

"""Hyperparameter Tuning the Decision Tree"""

param_grid={
    'criterion': ['entropy','gini'],  #the splitting of the tree
    'max_depth': [np.arange(1,21,1),None],  #the maximum depth of the tree
    'max_features': ['auto','sqrt','log2', None], #features to use to split the tree
    'max_leaf_nodes':np.arange(1,31)}    #the leaf nodes
tree_grid=GridSearchCV(decision_tree,param_grid=param_grid,cv=n_folds,scoring='accuracy',n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %time tree_grid.fit(X_train,y_train)

print("The best parameter is:\t",tree_grid.best_params_) #obtaining the best parameters based on the results of the GridSearchCV
print('\n')
print("The best estimator is:\t",tree_grid.best_estimator_) #obtaining the best estimator based on the results of the GridSearchCV
print('\n')
print("The best score is:\t",tree_grid.best_score_) #obtaining the best score based on the results of the GridSearchCV

decision_grid_model=tree_grid.best_estimator_
decision_grid_model.fit(X_train,y_train)  #fitting the best estimator with the training dataset

cross_val_score(decision_grid_model,X_train,y_train,cv=n_folds,scoring='accuracy').mean() #training accuracy after obtaining the best estimator and using it.

cross_val_score(decision_grid_model,X_train,y_train,cv=n_folds,scoring='precision').mean()
#precision score using the best parameters of the decision tree

y_train_pred=decision_grid_model.predict(X_train) #predicting the training dataset
decision_train_accuracy=metrics.accuracy_score(y_train,y_train_pred)
decision_train_precision=metrics.precision_score(y_train,y_train_pred) #precision score of the training dataset.

y_pred=decision_grid_model.predict(X_test)  #using the model to predict a new dataset and validating the model
decision_test_accuracy=metrics.accuracy_score(y_test,y_pred)  #calculating the model accuracy after validating it
decision_test_precision=metrics.precision_score(y_test, y_pred) #calculating the model precision after validating it

#visualizing the Decision Tree model
plt.figure(figsize=(20,12))
features_names=['Pclass','Age','SibSp','Parch','Fare','Embarked','Sex_female','Sex_male']
tree.plot_tree(decision_grid_model,filled=True,fontsize=15,feature_names=features_names,impurity=False)
plt.show()

"""# RANDOM FOREST CLASSIFIER"""

forest_clf=RandomForestClassifier(random_state=42)
forest_clf.fit(X_train,y_train)

evaluation(forest_clf, X_train, y_train, X_test, y_test)

cross_val_score(forest_clf,X_train,y_train, cv=n_folds,scoring='accuracy').mean()

cross_val_score(forest_clf,X_train,y_train, cv=n_folds,scoring='precision').mean()

param_grid={
    'n_estimators':np.arange(10,70,10),
    'criterion':['gini','entropy'],
    'max_depth':np.arange(1,8,1), 
    'max_features':np.arange(0,5),
    'max_leaf_nodes':np.arange(1,7,1)}
forest_grid=GridSearchCV(forest_clf,param_grid=param_grid,cv=5,scoring='accuracy',verbose=1)

# Commented out IPython magic to ensure Python compatibility.
# %time forest_grid.fit(X_train,y_train)

print("The best parameter is:\t",forest_grid.best_params_)
print('\n')
print("The best estimator is:\t",forest_grid.best_estimator_)
print('\n')
print("The best score is:\t",forest_grid.best_score_)

forest_grid_model=forest_grid.best_estimator_
forest_grid_model.fit(X_train,y_train)

evaluation(forest_grid_model, X_train, y_train, X_test, y_test)

cross_val_score(forest_grid_model, X_train, y_train, cv=n_folds, scoring='accuracy').mean()

cross_val_score(forest_grid_model, X_train, y_train, cv=n_folds, scoring='precision').mean()

y_train_pred=forest_grid_model.predict(X_train)
forest_train_accuracy=metrics.accuracy_score(y_train,y_train_pred)
forest_train_precision=metrics.precision_score(y_train,y_train_pred) #training precision accuracy using cross-validation

y_pred=forest_grid_model.predict(X_test)
forest_test_accuracy=metrics.accuracy_score(y_test,y_pred)
forest_test_precision=metrics.precision_score(y_test, y_pred)

"""# GradientBoostingClassifier"""

gradient_clf=GradientBoostingClassifier(random_state=42)
gradient_clf.fit(X_train,y_train)

evaluation(gradient_clf, X_train, y_train, X_test, y_test)

cross_val_score(gradient_clf,X_train,y_train, cv=n_folds,scoring='accuracy').mean() #the training accuracy of the model

cross_val_score(gradient_clf,X_train,y_train, cv=n_folds,scoring='precision').mean() #the precision accuracy of the model

"""Model Hypertuning"""

param_grid={'n_estimators':np.arange(10,160,10),
            'max_depth':np.arange(1,11,1),
            'max_features':np.arange(1,9),
            'subsample':np.arange(0.1,1),
            'learning_rate':np.arange(0.01,1)}
gradient_grid=GridSearchCV(gradient_clf, param_grid=param_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %time gradient_grid.fit(X_train,y_train)

print("The best parameter is:\t",gradient_grid.best_params_)
print('\n')
print("The best estimator is:\t",gradient_grid.best_estimator_)
print('\n')
print("The best score is:\t",gradient_grid.best_score_)

gradient_grid_model=gradient_grid.best_estimator_
gradient_grid_model.fit(X_train,y_train)

evaluation(gradient_grid_model, X_train, y_train, X_test, y_test)

cross_val_score(gradient_grid_model, X_train, y_train, cv=n_folds, scoring='accuracy').mean()

cross_val_score(gradient_grid_model, X_train, y_train, cv=n_folds, scoring='precision').mean()

y_train_pred=gradient_grid_model.predict(X_train)
gradient_train_accuracy=metrics.accuracy_score(y_train,y_train_pred)
gradient_train_precision=metrics.precision_score(y_train,y_train_pred) #training precision accuracy using cross-validation

y_pred=forest_grid_model.predict(X_test)
gradient_test_accuracy=metrics.accuracy_score(y_test,y_pred)
gradient_test_precision=metrics.precision_score(y_test, y_pred)

"""# XGBoost Classifier"""

from xgboost import XGBClassifier
xgb_clf=XGBClassifier(eval_metric='logloss',random_state=42)

xgb_clf.fit(X_train,y_train)    #training the model with training dataset.

evaluation(xgb_clf,X_train,y_train,X_test,y_test)

cross_val_score(xgb_clf,X_train,y_train, cv=n_folds,scoring='accuracy').mean() #the training accuracy of the model

cross_val_score(xgb_clf,X_train,y_train, cv=n_folds,scoring='precision').mean()

"""Model Hypertuning"""

param_grid={
    'booster':['gbtree','gblinear','dart'],
    'validate_parameters':[True,False],
    'eta':np.arange(0,1),
    'max_depth':np.arange(0,10,1)}
xgb_grid=GridSearchCV(xgb_clf,param_grid=param_grid,scoring='accuracy',cv=n_folds,verbose=1)

xgb_grid.fit(X_train,y_train)

print("The best parameter is:\t",xgb_grid.best_params_)
print('\n')
print("The best estimator is:\t",xgb_grid.best_estimator_)
print('\n')
print("The best score is:\t",xgb_grid.best_score_)

xgb_grid_model=xgb_grid.best_estimator_
xgb_grid_model.fit(X_train,y_train)

evaluation(xgb_grid_model, X_train, y_train, X_test, y_test)

cross_val_score(xgb_grid_model, X_train, y_train, cv=n_folds, scoring='accuracy').mean() 
# accuracy of the optimized model

cross_val_score(xgb_grid_model, X_train, y_train, cv=n_folds, scoring='precision').mean()

y_train_pred=xgb_grid_model.predict(X_train)
xgb_train_accuracy=metrics.accuracy_score(y_train,y_train_pred)
xgb_train_precision=metrics.precision_score(y_train,y_train_pred) #training precision accuracy using cross-validation

y_pred=xgb_grid_model.predict(X_test)
xgb_test_accuracy=metrics.accuracy_score(y_test,y_pred)
xgb_test_precision=metrics.precision_score(y_test, y_pred)

"""# **model comparison**"""

models=pd.DataFrame({
    'models':['KNeighborsClassifiers','DecisionTree','RandomForest','GradientBoost','Xgboost'],
    'Training_Accuracy':[knn_training_accuracy,decision_train_accuracy,forest_train_accuracy,gradient_train_accuracy,xgb_train_accuracy],
    'Testing Accuracy':[knn_test_accuracy,decision_test_accuracy,forest_test_accuracy,gradient_test_accuracy,xgb_test_accuracy],
    'Training Precision Score':[knn_train_precision,decision_train_precision,forest_train_precision,gradient_train_precision,xgb_train_precision],
    'Testing Precision Score':[knn_test_precision,decision_test_precision,forest_test_precision,gradient_test_precision,xgb_test_precision]})
#the above creates a dataframe of the metrics of the machine learning models.

models.set_index('models',inplace=True)   #setting the model name to be index and dropping the index name.
models.index.name=None

models   #displaying the dataframe

plt.figure(figsize=(15,8))

plt.subplot(1,2,1)
fig2=sns.barplot(data=models,x=models.index,y=models['Training_Accuracy'])
plt.title("Training Accuracy of models",size=18,color='white',backgroundcolor='black')

plt.subplot(1,2,2)
fig3=sns.barplot(data=models,x=models.index,y=models['Testing Accuracy'])
plt.title("Testing Accuracy of models",size=18,color='white',backgroundcolor='blue')
plt.show()   #vuisualizing the training and testing accuracy of the models

plt.figure(figsize=(15,8))

plt.subplot(1,2,1)
fig2=sns.barplot(data=models,x=models.index,y=models['Training Precision Score'])
plt.title("Training Precision of models",size=18,color='white',backgroundcolor='black')

plt.subplot(1,2,2)
fig3=sns.barplot(data=models,x=models.index,y=models['Testing Precision Score'])
plt.title("Testing precision of models",size=18,color='white',backgroundcolor='blue')
plt.show()  #vuisualizing the training and testing precision scores of the models

"""The XGBOOST CLASSIFIER WAS SELECTED AS IT HAS HIGH ACCURACY

# PREDICTING THE TEST DATA
"""

test_data.drop(['Name','Cabin'],axis=1,inplace=True)   #dropping the following columns dataset

encoder=LabelEncoder()
test_data['Embarked']=encoder.fit_transform(test_data['Embarked'])   #convering embarked object datatype into numerical datatype

final_model=xgb_grid_model.fit(X,y)  #training the model with the training dataset.

predictions=final_model.predict(test_data)  #predicting the test_data by using xgboost algorithm.

predictions #the predictions

our_submission=pd.DataFrame({'PassengerId': test_data.index, 'Survived': predictions})
#creating a dataframe for our predictions and their correspondinmg ids or index

our_submission #the dataframe

our_submission.to_csv('our_submission.csv', index=False)   #writing the results to a csv file and exporting them.

